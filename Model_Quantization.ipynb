{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qunatizing Pretrained DL Models in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_range_quantization(model_path,optimization):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_dynamic_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Successfuly created and saved Dynamic Range Quantized {} model\".format(model_path))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float16_quantization(model_path,optimization):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_f16_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "    converter.target_spec.supported_types = [tf.float16]   \n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(\"Successfuly created and saved Float16 Quantized {} model\".format(model_path))\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 224, 224, 3)\n",
    "        yield [data.astype(np.float32)]\n",
    "    \n",
    "def int8_quantization(model_path,optimization=None, integer_only = False):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_int8_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    \n",
    "    if integer_only:\n",
    "        # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "        save_path = model_path[:-3] + '_fullInt_quant.tflite'\n",
    "    \n",
    "    tflite_model_quant = converter.convert()\n",
    "    \n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model_quant)\n",
    "        \n",
    "    print(\"Successfuly created and saved Int8 Quantized {} model\".format(model_path))\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:48:40.270804: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "resnet_50 = keras.applications.resnet50.ResNet50(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "resnet_50.compiled_metrics = None\n",
    "resnet_50.save('Models/resnet_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = keras.applications.vgg16.VGG16(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "vgg16.compiled_metrics = None\n",
    "vgg16.save(\"Models/vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "mobilenet_v2.compiled_metrics = None\n",
    "mobilenet_v2.save('Models/mobilenet_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:49:00.259581: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:49:11.088206: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:49:11.088276: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:49:11.100412: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:49:14.209939: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:49:14.209959: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-09-16 16:49:14.448001: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/resnet_50.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:49:19.406017: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:49:19.406128: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:49:19.408694: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-16 16:49:40.594454: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:49:40.594579: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/vgg16.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:50:11.859569: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:50:11.859799: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:50:11.869816: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:50:13.146122: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:50:13.146138: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "2021-09-16 16:50:13.346968: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor mobilenetv2_1.00_224/Conv1/Conv2D because it has fewer than 1024 elements (864).\n",
      "2021-09-16 16:50:13.346987: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor mobilenetv2_1.00_224/expanded_conv_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/expanded_conv_depthwise/depthwise;mobilenetv2_1.00_224/block_5_project/Conv2D because it has fewer than 1024 elements (288).\n",
      "2021-09-16 16:50:13.346995: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor mobilenetv2_1.00_224/expanded_conv_project/Conv2D because it has fewer than 1024 elements (512).\n",
      "2021-09-16 16:50:13.347001: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor mobilenetv2_1.00_224/block_1_depthwise_BN/FusedBatchNormV3;mobilenetv2_1.00_224/block_1_depthwise/depthwise;mobilenetv2_1.00_224/block_12_p"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/mobilenet_v2.h5','deafult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float16 Qunatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:50:36.695296: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:50:36.695370: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:50:36.705608: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:50:39.874919: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:50:39.874937: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/resnet_50.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:50:46.060798: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:50:46.060896: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:50:46.063795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.005ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-09-16 16:51:01.470801: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:51:01.470823: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/vgg16.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:51:37.960597: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:51:37.960683: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:51:37.969550: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:51:39.217091: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:51:39.217109: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/mobilenet_v2.h5','deafult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Integer Qunatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:51:59.934415: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:51:59.934494: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:51:59.945956: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-16 16:52:03.807037: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:52:03.807056: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/resnet_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:52:29.849736: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:52:29.849799: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:52:29.851946: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-09-16 16:52:45.768623: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:52:45.768719: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:54:15.729887: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:54:15.729968: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:54:15.738763: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-16 16:54:16.897326: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:54:16.897345: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/mobilenet_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full integer qunatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:54:48.099508: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:54:48.099591: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:54:48.110494: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-16 16:54:51.097774: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:54:51.097790: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/resnet_50.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-16 16:55:19.330679: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:55:19.330738: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:55:19.333050: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-09-16 16:55:35.059126: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:55:35.059146: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/vgg16.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amishra162/Documents/Model-Optimization/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "2021-09-16 16:57:16.418806: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2021-09-16 16:57:16.418919: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2021-09-16 16:57:16.431898: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.003ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "2021-09-16 16:57:18.064288: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\n",
      "2021-09-16 16:57:18.064316: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/mobilenet_v2.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to Qunatize the model using checkpoint file or Frozen Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for testing has been taken from https://github.com/fastai/imagenette \n",
    "#which has images of 10 most common classes from IMagenet dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and unzipping official Tensorflow Models\n",
    "# Pretrained models : https://github.com/tensorflow/models/tree/master/research/slim\n",
    "# We will be using Resnet50 in this example\n",
    "!wget http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "!tar -xf resnet_v1_50_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Frozen graph from ckpt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "from tf_slim.nets import resnet_v1\n",
    "import tf_slim as slim\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# Create graph\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "        net, end_points = resnet_v1.resnet_v1_50(inputs, num_classes=1000,is_training=False)\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "output_node_names = (\"resnet_v1_50/pool5\")\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "        saver.restore(sess, 'resnet_v1_50.ckpt')\n",
    "        representation_tensor = sess.graph.get_tensor_by_name('resnet_v1_50/pool5:0') \n",
    "        tf.compat.v1.train.write_graph(sess.graph_def,'./','resnet_v1_50.pbtxt')\n",
    "\n",
    "        tensor_name_list = [tensor.name for tensor in tf.compat.v1.get_default_graph().as_graph_def().node]\n",
    "        #print([tensor_name for tensor_name in tensor_name_list])\n",
    "        output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "                    sess, # The session is used to retrieve the weights\n",
    "                    tf.compat.v1.get_default_graph().as_graph_def(), \n",
    "                    output_node_names.split(\",\") \n",
    "                )\n",
    "        with tf.compat.v1.gfile.GFile(\"resnet_v1_50.pb\", \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of post-training quantization statically quantizes only the weights from floating point to integer, which has 8-bits of precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_pth = \"resnet_v1_50.pb\"\n",
    "# Convert the model\n",
    "#converter = tf.lite.TFLiteConverter.from_frozen_graph(saved_model_dir) # path to the SavedModel directory\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Int8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 224, 224, 3)\n",
    "        yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_int8.tflite', 'wb') as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer with Float Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_fallback.tflite', 'wb') as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float16 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_float_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_float16.tflite', 'wb') as f:\n",
    "    f.write(tflite_float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Model-Optimization",
   "language": "python",
   "name": "model-optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
