{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qunatizing Pretrained DL Models in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_range_quantization(model_path,optimization):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_dynamic_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"Successfuly created and saved Dynamic Range Quantized {} model\".format(model_path))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float16_quantization(model_path,optimization):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_f16_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "    converter.target_spec.supported_types = [tf.float16]   \n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(\"Successfuly created and saved Float16 Quantized {} model\".format(model_path))\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():  \n",
    "    count =0\n",
    "    for i in (os.listdir('val/n01440764/')):\n",
    "        \n",
    "        if count<100:\n",
    "            count += 1\n",
    "            f_name = './val/n01440764/'+i\n",
    "            img = (tf.keras.preprocessing.image.load_img(f_name,target_size=(224,224)))\n",
    "            img = (tf.keras.preprocessing.image.img_to_array(img))\n",
    "            img = np.expand_dims(img,axis=0)#/255\n",
    "            yield [img]\n",
    "    \n",
    "def int8_quantization(model_path,optimization=None, integer_only = False):\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    save_path = model_path[:-3] + '_int8_quant.tflite'\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if optimization == \"size\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "    elif optimization == \"latency\":\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\n",
    "    else:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    \n",
    "    if integer_only:\n",
    "        # Ensure that if any ops can't be quantized, the converter throws an error\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        # Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "        save_path = model_path[:-3] + '_fullInt_quant.tflite'\n",
    "    \n",
    "    tflite_model_quant = converter.convert()\n",
    "    \n",
    "    # Save the model.\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model_quant)\n",
    "        \n",
    "    print(\"Successfuly created and saved Int8 Quantized {} model\".format(model_path))\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet_50 = keras.applications.resnet50.ResNet50(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "resnet_50.compiled_metrics = None\n",
    "resnet_50.save('Models/resnet_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = keras.applications.vgg16.VGG16(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "vgg16.compiled_metrics = None\n",
    "vgg16.save(\"Models/vgg16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    include_top=True, weights='imagenet',\n",
    "    input_shape=(224,224,3))\n",
    "mobilenet_v2.compiled_metrics = None\n",
    "mobilenet_v2.save('Models/mobilenet_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 16:34:24.379178: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:34:24.379196: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:34:24.379638: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp3iw26z26\n",
      "2022-10-01 16:34:24.396577: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:34:24.396589: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp3iw26z26\n",
      "2022-10-01 16:34:24.446765: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-10-01 16:34:24.463212: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:34:24.473285: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-10-01 16:34:24.901477: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp3iw26z26\n",
      "2022-10-01 16:34:25.046210: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 666568 microseconds.\n",
      "2022-10-01 16:34:25.297902: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:263] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/resnet_50.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n",
      "2022-10-01 16:34:34.517906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:34:34.517922: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:34:34.518008: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeo23mavn\n",
      "2022-10-01 16:34:34.520241: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:34:34.520247: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeo23mavn\n",
      "2022-10-01 16:34:34.528118: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:34:34.957319: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeo23mavn\n",
      "2022-10-01 16:34:34.978002: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 459995 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/vgg16.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Dynamic Range Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 16:37:10.886066: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:37:10.886079: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:37:10.886176: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeisv2qie\n",
      "2022-10-01 16:37:10.902274: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:37:10.902287: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeisv2qie\n",
      "2022-10-01 16:37:10.961604: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:37:11.264689: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpeisv2qie\n",
      "2022-10-01 16:37:11.387419: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 501247 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_range_quantization('Models/mobilenet_v2.h5','deafult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float16 Qunatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "2022-10-01 16:37:50.877909: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:37:50.877926: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:37:50.878011: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpsm_oe1py\n",
      "2022-10-01 16:37:50.895946: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:37:50.895962: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpsm_oe1py\n",
      "2022-10-01 16:37:50.963239: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:37:51.402692: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpsm_oe1py\n",
      "2022-10-01 16:37:51.545064: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 667053 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/resnet_50.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n",
      "2022-10-01 16:37:59.194393: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:37:59.194406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:37:59.194486: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpxfn8kj_0\n",
      "2022-10-01 16:37:59.196646: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:37:59.196653: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpxfn8kj_0\n",
      "2022-10-01 16:37:59.204633: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:37:59.634396: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpxfn8kj_0\n",
      "2022-10-01 16:37:59.655974: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 461488 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/vgg16.h5','deafult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Float16 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 16:40:23.021858: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:40:23.021871: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:40:23.021950: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpabc6aa3j\n",
      "2022-10-01 16:40:23.038099: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:40:23.038115: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpabc6aa3j\n",
      "2022-10-01 16:40:23.098586: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:40:23.399164: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpabc6aa3j\n",
      "2022-10-01 16:40:23.522854: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 500901 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float16_quantization('Models/mobilenet_v2.h5','deafult')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Integer Qunatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Float Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 16:41:02.729106: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:41:02.729120: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:41:02.729203: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpiqnhhdqh\n",
      "2022-10-01 16:41:02.747308: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:41:02.747324: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpiqnhhdqh\n",
      "2022-10-01 16:41:02.816614: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:41:03.257956: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpiqnhhdqh\n",
      "2022-10-01 16:41:03.402226: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 673023 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/resnet_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 16:41:33.231885: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:41:33.231898: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:41:33.231996: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpweqxqwdg\n",
      "2022-10-01 16:41:33.234412: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:41:33.234422: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpweqxqwdg\n",
      "2022-10-01 16:41:33.242838: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:41:33.687503: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpweqxqwdg\n",
      "2022-10-01 16:41:33.708952: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 476955 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 16:44:33.069914: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:44:33.069928: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:44:33.070008: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpqzipqiko\n",
      "2022-10-01 16:44:33.086691: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:44:33.086705: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpqzipqiko\n",
      "2022-10-01 16:44:33.155075: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:44:33.471347: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpqzipqiko\n",
      "2022-10-01 16:44:33.601236: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 531229 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 0, output_inference_type: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/mobilenet_v2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full integer qunatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 16:45:20.963767: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:45:20.963780: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:45:20.963860: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpfs4dszb2\n",
      "2022-10-01 16:45:20.981876: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:45:20.981889: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpfs4dszb2\n",
      "2022-10-01 16:45:21.053676: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:45:21.495120: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpfs4dszb2\n",
      "2022-10-01 16:45:21.638820: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 674960 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/resnet_50.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/resnet_50.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 16:45:50.463378: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 16:45:50.463395: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 16:45:50.463477: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp51vmb0yl\n",
      "2022-10-01 16:45:50.465677: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 16:45:50.465684: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp51vmb0yl\n",
      "2022-10-01 16:45:50.473589: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 16:45:50.903454: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmp51vmb0yl\n",
      "2022-10-01 16:45:50.923443: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 459966 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/vgg16.h5 model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/vgg16.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n",
      "/Users/amishra162/Documents/Coursera/work/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-10-01 17:44:05.547334: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-10-01 17:44:05.547353: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-10-01 17:44:05.547448: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpy0e52dy_\n",
      "2022-10-01 17:44:05.563656: I tensorflow/cc/saved_model/reader.cc:81] Reading meta graph with tags { serve }\n",
      "2022-10-01 17:44:05.563670: I tensorflow/cc/saved_model/reader.cc:122] Reading SavedModel debug info (if present) from: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpy0e52dy_\n",
      "2022-10-01 17:44:05.626533: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-10-01 17:44:05.930963: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /var/folders/hx/8ktl0wt56q7dx073nt1x6dsm0000gp/T/tmpy0e52dy_\n",
      "2022-10-01 17:44:06.054479: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 507032 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly created and saved Int8 Quantized Models/mobilenet_v2.h5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int8_quantization('Models/mobilenet_v2.h5', integer_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to Qunatize the model using checkpoint file or Frozen Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset for testing has been taken from https://github.com/fastai/imagenette \n",
    "#which has images of 10 most common classes from IMagenet dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading and unzipping official Tensorflow Models\n",
    "# Pretrained models : https://github.com/tensorflow/models/tree/master/research/slim\n",
    "# We will be using Resnet50 in this example\n",
    "!wget http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "!tar -xf resnet_v1_50_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Frozen graph from ckpt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.tools import freeze_graph\n",
    "from tf_slim.nets import resnet_v1\n",
    "import tf_slim as slim\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "# Create graph\n",
    "inputs = tf.compat.v1.placeholder(tf.float32, shape=[1, 224, 224, 3])\n",
    "with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "        net, end_points = resnet_v1.resnet_v1_50(inputs, num_classes=1000,is_training=False)\n",
    "\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "output_node_names = (\"resnet_v1_50/pool5\")\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "        saver.restore(sess, 'resnet_v1_50.ckpt')\n",
    "        representation_tensor = sess.graph.get_tensor_by_name('resnet_v1_50/pool5:0') \n",
    "        tf.compat.v1.train.write_graph(sess.graph_def,'./','resnet_v1_50.pbtxt')\n",
    "\n",
    "        tensor_name_list = [tensor.name for tensor in tf.compat.v1.get_default_graph().as_graph_def().node]\n",
    "        #print([tensor_name for tensor_name in tensor_name_list])\n",
    "        output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "                    sess, # The session is used to retrieve the weights\n",
    "                    tf.compat.v1.get_default_graph().as_graph_def(), \n",
    "                    output_node_names.split(\",\") \n",
    "                )\n",
    "        with tf.compat.v1.gfile.GFile(\"resnet_v1_50.pb\", \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of post-training quantization statically quantizes only the weights from floating point to integer, which has 8-bits of precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_pth = \"resnet_v1_50.pb\"\n",
    "# Convert the model\n",
    "#converter = tf.lite.TFLiteConverter.from_frozen_graph(saved_model_dir) # path to the SavedModel directory\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Int8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "        data = np.random.rand(1, 224, 224, 3)\n",
    "        yield [data.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_int8_model = converter.convert()\n",
    "\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_int8.tflite', 'wb') as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integer with Float Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_fallback.tflite', 'wb') as f:\n",
    "    f.write(tflite_int8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Float16 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_graph_pth,\n",
    "                                                                input_arrays=['Placeholder'],\n",
    "                                                                input_shapes={'Placeholder' : [1, 224, 224,3]},\n",
    "                                                                output_arrays=[\"resnet_v1_50/pool5\"])\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_float_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('resnet_v1_50_float16.tflite', 'wb') as f:\n",
    "    f.write(tflite_float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('work': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbb4c99ba6a158315ca45ade86b338397709de922c2faeb37289f32131d55cc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
